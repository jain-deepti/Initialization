# Initialization

Training your neural network requires specifying an initial value of the weights. A well-chosen initialization method helps the learning process.This notebook, explores a few different initializations, including random, zeros, and He initialization, and sees how each leads to different results.

A well-chosen initialization can:

Speed up the convergence of gradient descent
Increase the odds of gradient descent converging to a lower training (and generalization) error
